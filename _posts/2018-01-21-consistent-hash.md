---
layout: post
title: 何为一致性哈希算法 
category: 分布式
tag: [Algorithm]
---

### 问题

哈希函数是一类将任意大小的数据映射到固定大小数据的运算指令集。比较常用的是内存哈希表，指定一个任意大小的key，经过hash运算后能快速定位到内存中的value。

Web应用中，为提高系统吞吐率，降低响应时间，通常会用到缓存服务。而为了提高缓存服务的可用性，缓存通常以集群的方式部署，由多台机器同时提供服务，保证任意一台机器挂掉，不会影响整体服务可用性。然而，缓存机器挂掉，不可能对系统没有任何影响，最常见的就是宕机的缓存数据丢失，导致后端系统压力大，严重的话可能会带来雪崩。那么如何尽量降低这一影响呢？假设有n台缓存服务器，其中任意一台挂掉，就会有 1/n 的缓存请求穿透到后端应用，那么其实要做的就是评估这 1/n的流量后端应用是否能抗住。如果无法抗住，就增加缓存节点的数量，否则保持不变或者减少节点数量。额外的还需冗余一定数量的节点，保证挂掉几台服务也能保持稳定。通过这种动态调节n的大小，保证了服务的稳定性，同时成本可控。然而问题是，动态增删节点，如何保证缓存数据miss率最小？

### 取模哈希
根据上面的定义，取模哈希也是一种hash函数，符合将任意大小的数字映射到指定范围内这一性质，如任意数字 n，对10取模，结果总是在0到9之间，公式为：

	h = Hash(key) % N

在缓存集群中应用这一hash函数，结果会怎样呢？
显然，任意的数据经过hash后，能够均分地分布到节点之间，但 n 变化之后，大多数key哈希到了不同的节点，导致缓存大规模失效。

### 一致性哈希
取模哈希的缺点是，每台服务器只负责key经过哈希后的一个值的缓存服务，而一致性hash的改进是，每台服务器负责key经过hash后的一个值范围的的缓存服务。如下图：

![mod_hash](/assets/images/mod_hash.png)

![consistent_hash](/assets/images/consistent_hash.png)

如果这个范围拆分的合理，那么节点的变更，最终只会导致部分数据重新分布（保证了key分布的单调性）。那么如何拆分的更合理呢？

在实际实现中，这个范围被称做key space，它包含了所有key被hash后的值，如下图，假设key space的范围为 0 ~ 2^32-1。

key被哈希后，对应value存储于所在范围顺时针方向的下个节点中。

![consistent_hash](/assets/images/consistent_hash1.jpg)

假设其中的cache2挂掉，只需要将cache2中key范围为cache1到cache2之间的数据移动到cache3上，cache2到cache3、cache3到cache1之间的数据是不用动的，从而保证了较少数据的迁移。同理如果增加了cache4，只需将cache1中key范围为cache1到cache4的数据移动。

为了提高key的分布均匀性，通常key spacke中的节点不是真实的节点，而是虚拟出来的。真实节点和虚拟节点之间存在一个映射关系。



